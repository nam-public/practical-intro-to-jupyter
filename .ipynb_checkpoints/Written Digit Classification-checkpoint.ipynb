{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "517653bdcd8caa01",
   "metadata": {},
   "source": [
    "# Importing Libraries\n",
    "As usual with our work, we shall start by importing the libraries we need to work with our data.\n",
    "To ensure you have all the libraries we shall use installed, make sure to run the following command in your command line \n",
    "```pip install tensorflow matplotlib seaborn numpy pandas``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9a38f4f19676134",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-24T16:25:31.564663700Z",
     "start_time": "2023-09-24T16:25:31.552276800Z"
    }
   },
   "outputs": [],
   "source": [
    "import keras.src.initializers.initializers\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import math\n",
    "import datetime\n",
    "import platform\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1501e6dac6f61156",
   "metadata": {},
   "source": [
    "# Loading Data\n",
    "Let's go ahead and load our data. Since in this case our data is split into two separate files, we won't need to manually split it using a train_test_split. \n",
    "Just to make it clear with the data we are loading in being called 'wdr-test.csv and wdr-train.csv'. The 'wdr' here stands for Written Digit Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3569105bcc392c61",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-24T13:53:17.304025200Z",
     "start_time": "2023-09-24T13:53:14.847302300Z"
    }
   },
   "outputs": [],
   "source": [
    "#Load our dataset\n",
    "train_df = pd.read_csv('wdr-train.csv')\n",
    "test_df = pd.read_csv('wdr-test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114cefadb758a3a4",
   "metadata": {},
   "source": [
    "Now, once again after we have loaded our dataset we shall take a minute to try and understand it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "264e3011b592e932",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-24T13:53:19.271942500Z",
     "start_time": "2023-09-24T13:53:19.259861600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
      "0          1       0       0       0       0       0       0       0       0   \n",
      "1          0       0       0       0       0       0       0       0       0   \n",
      "2          1       0       0       0       0       0       0       0       0   \n",
      "3          4       0       0       0       0       0       0       0       0   \n",
      "4          0       0       0       0       0       0       0       0       0   \n",
      "...      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
      "41995      0       0       0       0       0       0       0       0       0   \n",
      "41996      1       0       0       0       0       0       0       0       0   \n",
      "41997      7       0       0       0       0       0       0       0       0   \n",
      "41998      6       0       0       0       0       0       0       0       0   \n",
      "41999      9       0       0       0       0       0       0       0       0   \n",
      "\n",
      "       pixel8  ...  pixel774  pixel775  pixel776  pixel777  pixel778  \\\n",
      "0           0  ...         0         0         0         0         0   \n",
      "1           0  ...         0         0         0         0         0   \n",
      "2           0  ...         0         0         0         0         0   \n",
      "3           0  ...         0         0         0         0         0   \n",
      "4           0  ...         0         0         0         0         0   \n",
      "...       ...  ...       ...       ...       ...       ...       ...   \n",
      "41995       0  ...         0         0         0         0         0   \n",
      "41996       0  ...         0         0         0         0         0   \n",
      "41997       0  ...         0         0         0         0         0   \n",
      "41998       0  ...         0         0         0         0         0   \n",
      "41999       0  ...         0         0         0         0         0   \n",
      "\n",
      "       pixel779  pixel780  pixel781  pixel782  pixel783  \n",
      "0             0         0         0         0         0  \n",
      "1             0         0         0         0         0  \n",
      "2             0         0         0         0         0  \n",
      "3             0         0         0         0         0  \n",
      "4             0         0         0         0         0  \n",
      "...         ...       ...       ...       ...       ...  \n",
      "41995         0         0         0         0         0  \n",
      "41996         0         0         0         0         0  \n",
      "41997         0         0         0         0         0  \n",
      "41998         0         0         0         0         0  \n",
      "41999         0         0         0         0         0  \n",
      "\n",
      "[42000 rows x 785 columns]\n"
     ]
    }
   ],
   "source": [
    "print(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f40a31f69a26deb",
   "metadata": {},
   "source": [
    "Alright, so here it appears the data has 42,000 items and 785 columns. That's cool and it appears that every single digit we are trying to classify is represented by 785 columns. \n",
    "Next let's do a simple shape print to confirm this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "298f62fb40d37b34",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-24T13:54:03.683800400Z",
     "start_time": "2023-09-24T13:54:03.676468100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape:  (42000, 785)\n",
      "test shape:  (28000, 784)\n"
     ]
    }
   ],
   "source": [
    "print('train shape: ', train_df.shape)\n",
    "print('test shape: ', test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d760c9241b5a132",
   "metadata": {},
   "source": [
    "The shape here shows us that while the training data has 785 pixel per character, it would appear the testing data frame has 784 instead. \n",
    "Next, let's work on splitting our data into the inputs and outputs. Again, the X and Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa929234800328fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df.iloc[:, 1:785] #Here we are selecting all columns (:) and columns from 1-785\n",
    "Y = train_df.iloc[:, 0] #Then we select the first column which is the identity of the numbers\n",
    "# Let's pick out our test data as well\n",
    "X_test = test_df.iloc[:,1:784] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ebc23178581736",
   "metadata": {},
   "source": [
    "Next we're going to perform our main objective today, we're going to visualize large data. Now in our case, it's not that the data is exceptionally large, rather it's that the data has very many features. \n",
    "\n",
    "Because our data is made up of 785 columns, it's extremely high dimensional data. This means that by simply printing the data, we can't really find a way to understand it. This is where dimensionality reduction comes in handy. \n",
    "\n",
    "We are going to use t_SNE to help us bring doing down our dataset into 2 axes. Then we shall use a scatter plot to visualize this data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b73db608c1d6990e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-24T16:23:20.790608400Z",
     "start_time": "2023-09-24T16:20:22.298289100Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m X_tsn \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m255\u001b[39m\n\u001b[0;32m      2\u001b[0m tsne \u001b[38;5;241m=\u001b[39m TSNE()\n\u001b[1;32m----> 3\u001b[0m tsne_res \u001b[38;5;241m=\u001b[39m tsne\u001b[38;5;241m.\u001b[39mfit_transform(X_tsn)\n\u001b[0;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m14\u001b[39m,\u001b[38;5;241m12\u001b[39m))\n\u001b[0;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mscatter(tsne_res[:,\u001b[38;5;241m0\u001b[39m], tsne_res[:,\u001b[38;5;241m1\u001b[39m],c\u001b[38;5;241m=\u001b[39mY,s\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[1;32m~\\miniconda3\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:157\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 157\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    159\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    160\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    161\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    162\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    163\u001b[0m         )\n",
      "File \u001b[1;32m~\\miniconda3\\Lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\miniconda3\\Lib\\site-packages\\sklearn\\manifold\\_t_sne.py:1111\u001b[0m, in \u001b[0;36mTSNE.fit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m   1090\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fit X into an embedded space and return that transformed output.\u001b[39;00m\n\u001b[0;32m   1091\u001b[0m \n\u001b[0;32m   1092\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;124;03m    Embedding of the training data in low-dimensional space.\u001b[39;00m\n\u001b[0;32m   1109\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1110\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params_vs_input(X)\n\u001b[1;32m-> 1111\u001b[0m embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(X)\n\u001b[0;32m   1112\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_ \u001b[38;5;241m=\u001b[39m embedding\n\u001b[0;32m   1113\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_\n",
      "File \u001b[1;32m~\\miniconda3\\Lib\\site-packages\\sklearn\\manifold\\_t_sne.py:952\u001b[0m, in \u001b[0;36mTSNE._fit\u001b[1;34m(self, X, skip_num_points)\u001b[0m\n\u001b[0;32m    945\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    946\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[t-SNE] Indexed \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m samples in \u001b[39m\u001b[38;5;132;01m{:.3f}\u001b[39;00m\u001b[38;5;124ms...\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    947\u001b[0m             n_samples, duration\n\u001b[0;32m    948\u001b[0m         )\n\u001b[0;32m    949\u001b[0m     )\n\u001b[0;32m    951\u001b[0m t0 \u001b[38;5;241m=\u001b[39m time()\n\u001b[1;32m--> 952\u001b[0m distances_nn \u001b[38;5;241m=\u001b[39m knn\u001b[38;5;241m.\u001b[39mkneighbors_graph(mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdistance\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    953\u001b[0m duration \u001b[38;5;241m=\u001b[39m time() \u001b[38;5;241m-\u001b[39m t0\n\u001b[0;32m    954\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose:\n",
      "File \u001b[1;32m~\\miniconda3\\Lib\\site-packages\\sklearn\\neighbors\\_base.py:986\u001b[0m, in \u001b[0;36mKNeighborsMixin.kneighbors_graph\u001b[1;34m(self, X, n_neighbors, mode)\u001b[0m\n\u001b[0;32m    983\u001b[0m     A_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones(n_queries \u001b[38;5;241m*\u001b[39m n_neighbors)\n\u001b[0;32m    985\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdistance\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 986\u001b[0m     A_data, A_ind \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkneighbors(X, n_neighbors, return_distance\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    987\u001b[0m     A_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mravel(A_data)\n\u001b[0;32m    989\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\miniconda3\\Lib\\site-packages\\sklearn\\neighbors\\_base.py:822\u001b[0m, in \u001b[0;36mKNeighborsMixin.kneighbors\u001b[1;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[0;32m    815\u001b[0m use_pairwise_distances_reductions \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    816\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbrute\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    817\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m ArgKmin\u001b[38;5;241m.\u001b[39mis_usable_for(\n\u001b[0;32m    818\u001b[0m         X \u001b[38;5;28;01mif\u001b[39;00m X \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meffective_metric_\n\u001b[0;32m    819\u001b[0m     )\n\u001b[0;32m    820\u001b[0m )\n\u001b[0;32m    821\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_pairwise_distances_reductions:\n\u001b[1;32m--> 822\u001b[0m     results \u001b[38;5;241m=\u001b[39m ArgKmin\u001b[38;5;241m.\u001b[39mcompute(\n\u001b[0;32m    823\u001b[0m         X\u001b[38;5;241m=\u001b[39mX,\n\u001b[0;32m    824\u001b[0m         Y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_X,\n\u001b[0;32m    825\u001b[0m         k\u001b[38;5;241m=\u001b[39mn_neighbors,\n\u001b[0;32m    826\u001b[0m         metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meffective_metric_,\n\u001b[0;32m    827\u001b[0m         metric_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meffective_metric_params_,\n\u001b[0;32m    828\u001b[0m         strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    829\u001b[0m         return_distance\u001b[38;5;241m=\u001b[39mreturn_distance,\n\u001b[0;32m    830\u001b[0m     )\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[0;32m    833\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbrute\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetric \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprecomputed\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m issparse(X)\n\u001b[0;32m    834\u001b[0m ):\n\u001b[0;32m    835\u001b[0m     results \u001b[38;5;241m=\u001b[39m _kneighbors_from_graph(\n\u001b[0;32m    836\u001b[0m         X, n_neighbors\u001b[38;5;241m=\u001b[39mn_neighbors, return_distance\u001b[38;5;241m=\u001b[39mreturn_distance\n\u001b[0;32m    837\u001b[0m     )\n",
      "File \u001b[1;32m~\\miniconda3\\Lib\\site-packages\\sklearn\\metrics\\_pairwise_distances_reduction\\_dispatcher.py:259\u001b[0m, in \u001b[0;36mArgKmin.compute\u001b[1;34m(cls, X, Y, k, metric, chunk_size, metric_kwargs, strategy, return_distance)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute the argkmin reduction.\u001b[39;00m\n\u001b[0;32m    179\u001b[0m \n\u001b[0;32m    180\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;124;03mreturns.\u001b[39;00m\n\u001b[0;32m    257\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m Y\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mfloat64:\n\u001b[1;32m--> 259\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ArgKmin64\u001b[38;5;241m.\u001b[39mcompute(\n\u001b[0;32m    260\u001b[0m         X\u001b[38;5;241m=\u001b[39mX,\n\u001b[0;32m    261\u001b[0m         Y\u001b[38;5;241m=\u001b[39mY,\n\u001b[0;32m    262\u001b[0m         k\u001b[38;5;241m=\u001b[39mk,\n\u001b[0;32m    263\u001b[0m         metric\u001b[38;5;241m=\u001b[39mmetric,\n\u001b[0;32m    264\u001b[0m         chunk_size\u001b[38;5;241m=\u001b[39mchunk_size,\n\u001b[0;32m    265\u001b[0m         metric_kwargs\u001b[38;5;241m=\u001b[39mmetric_kwargs,\n\u001b[0;32m    266\u001b[0m         strategy\u001b[38;5;241m=\u001b[39mstrategy,\n\u001b[0;32m    267\u001b[0m         return_distance\u001b[38;5;241m=\u001b[39mreturn_distance,\n\u001b[0;32m    268\u001b[0m     )\n\u001b[0;32m    270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m Y\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mfloat32:\n\u001b[0;32m    271\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ArgKmin32\u001b[38;5;241m.\u001b[39mcompute(\n\u001b[0;32m    272\u001b[0m         X\u001b[38;5;241m=\u001b[39mX,\n\u001b[0;32m    273\u001b[0m         Y\u001b[38;5;241m=\u001b[39mY,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    279\u001b[0m         return_distance\u001b[38;5;241m=\u001b[39mreturn_distance,\n\u001b[0;32m    280\u001b[0m     )\n",
      "File \u001b[1;32msklearn\\metrics\\_pairwise_distances_reduction\\_argkmin.pyx:90\u001b[0m, in \u001b[0;36msklearn.metrics._pairwise_distances_reduction._argkmin.ArgKmin64.compute\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\miniconda3\\Lib\\site-packages\\threadpoolctl.py:440\u001b[0m, in \u001b[0;36m_ThreadpoolLimiter.__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m    437\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__enter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    438\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m--> 440\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mtype\u001b[39m, value, traceback):\n\u001b[0;32m    441\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrestore_original_limits()\n\u001b[0;32m    443\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    444\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrap\u001b[39m(\u001b[38;5;28mcls\u001b[39m, controller, \u001b[38;5;241m*\u001b[39m, limits\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, user_api\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X_tsn = X/255\n",
    "tsne = TSNE()\n",
    "tsne_res = tsne.fit_transform(X_tsn)\n",
    "plt.figure(figsize=(14,12))\n",
    "plt.scatter(tsne_res[:,0], tsne_res[:,1],c=Y,s=2)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e219b3f4e66a906",
   "metadata": {},
   "source": [
    "Think of the space of the graph as spaces that represent locations of pixel. The graph shows then, that specific numbers use specific pixel more than other numbers do, aside from a few outliers. This forms the basis for our AI analysis to begin. \n",
    "\n",
    "Let's now move on to creating a validation and training set for our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5cd3856bc37c5be0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-24T16:37:01.796584100Z",
     "start_time": "2023-09-24T16:37:01.605765500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape:  (33600, 784)\n",
      "X_validation shape:  (8400, 784)\n",
      "Y_train shape:  (33600,)\n",
      "Y_validation shape:  (8400,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_validation, Y_train, Y_validation = train_test_split(X, Y, test_size=0.2, random_state=1212)\n",
    "print(\"X_train shape: \",X_train.shape)\n",
    "print(\"X_validation shape: \",X_validation.shape)\n",
    "print(\"Y_train shape: \",Y_train.shape)\n",
    "print(\"Y_validation shape: \",Y_validation.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b240deb1cb2028f7",
   "metadata": {},
   "source": [
    "Now we want to convert our data into arrays. Because our data is an array of images, we need to convert each image from a 1D list of images to a 2D grid of the same image data but now arranged in a similar way pixels would. \n",
    "\n",
    "As you can see from our logs above, the test data frame contains 33600 items while the training set contains 8400 and each have 784 items. The number 784 is actually a square number, so we can tell that the images are square images, having an area of 784 meaning a height and width of 28x28. To change our data into an array we shall use numpy reshape which takes in 3 params which our case will correspond to (image_count, width, length). Then we proceed to do the same for the other items in our array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eac338c01b542c0b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-24T16:42:56.056185200Z",
     "start_time": "2023-09-24T16:42:56.040428900Z"
    }
   },
   "outputs": [],
   "source": [
    "x_train_arr = X_train.to_numpy().reshape(33600,28,28)\n",
    "y_train_arr = Y_train.values\n",
    "x_validation_arr = X_validation.to_numpy().reshape(8400,28,28)\n",
    "y_validation_arr = Y_validation.values\n",
    "x_test_arr = test_df.to_numpy().reshape(28000, 28,28)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1d7c6539b9d777",
   "metadata": {},
   "source": [
    "Now let's save our data shape to some variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d715f20c86c71b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "(_, IMAGE_WIDTH, IMAGE_HEIGHT) = x_train_arr.shape\n",
    "IMAGE_CHANNELS = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129135b18c986ba5",
   "metadata": {},
   "source": [
    "Now, what's the point of organizing our data into image space if we can't visualize it? Well, we can. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d396ba5fe512533",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-24T16:56:06.385886600Z",
     "start_time": "2023-09-24T16:56:06.292441600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_train_arr[2], cmap=plt.cm.binary)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87d6b447ada3b61",
   "metadata": {},
   "source": [
    "Alright, now that we have our images set out into grids that can be visualized, we need make one last change. Images normally come in as a grid of pixels and that's why we have organized them into a 2D grid. However, each pixel normally has 3 channels (RGB) and thus we need yet another dimension to our data, 3 additional channels to which we can add values for the amount of RGB in each image. Luckily, this is easy to achieve. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4296a2d85fcee9a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-24T16:59:16.067600100Z",
     "start_time": "2023-09-24T16:59:16.063813700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train_with_channels: (33600, 28, 28, 1)\n",
      "x_validation_with_channels: (8400, 28, 28, 1)\n",
      "x_test_with_channels: (28000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "x_train_with_channels = x_train_arr.reshape(\n",
    "    x_train_arr.shape[0],  # Number of images (assuming the first dimension is the number of images)\n",
    "    IMAGE_WIDTH,            # Width of each image\n",
    "    IMAGE_HEIGHT,           # Height of each image\n",
    "    IMAGE_CHANNELS          # Number of channels (e.g., 1 for grayscale, 3 for RGB)\n",
    ")\n",
    "\n",
    "x_validation_with_channels = x_validation_arr.reshape(\n",
    "    x_validation_arr.shape[0],\n",
    "    IMAGE_WIDTH,\n",
    "    IMAGE_HEIGHT,\n",
    "    IMAGE_CHANNELS\n",
    ")\n",
    "x_test_with_channels = x_test_arr.reshape(\n",
    "    x_test_arr.shape[0],\n",
    "    IMAGE_WIDTH,\n",
    "    IMAGE_HEIGHT,\n",
    "    IMAGE_CHANNELS\n",
    ")\n",
    "\n",
    "print('x_train_with_channels:', x_train_with_channels.shape)\n",
    "print('x_validation_with_channels:', x_validation_with_channels.shape)\n",
    "print('x_test_with_channels:', x_test_with_channels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322dd0a023e2d810",
   "metadata": {},
   "source": [
    "# Building our Model\n",
    "We're finally here. Now that we have understood how our AI can classify data, we can choose a suitable model and start to train it. Because our problem is one of image recognition, there is no doubt that our best candidate here is a Convolutional Neural Network. \n",
    "\n",
    "Since we're working with a  convolutional model this time, the setup process for our model is going to be a bit more complex as it will have multiple layers that I will outline. \n",
    "\n",
    "We shall have two convolutional layers to try and extract the features from the data set. Then we shall apply a dropout to reduce the dependence of the model on the data, and finally we pass the data through a Dense layer to classify the data. The output is a 10-dimensional vector where every item represents a number from 0-10. Let's take a look at what this looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "31b8f31b767c49b4",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-24T19:05:34.127028300Z"
    }
   },
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "# Add a convolutional layer to the neural network model\n",
    "model.add(tf.keras.layers.Convolution2D(\n",
    "    input_shape=(IMAGE_WIDTH, IMAGE_HEIGHT, IMAGE_CHANNELS),  # Input shape for images\n",
    "    kernel_size=5,  # Size of the convolutional kernel (5x5)\n",
    "    filters=8,  # Number of filters (output channels)\n",
    "    strides=1,  # Stride of the convolution operation (1 pixel at a time)\n",
    "    activation=tf.keras.activations.relu,  # Rectified Linear Unit (ReLU) activation\n",
    "    kernel_initializer=tf.keras.initializers.VarianceScaling()  # Weight initialization\n",
    "))\n",
    "\n",
    "# Add a MaxPooling layer to the neural network model\n",
    "model.add(tf.keras.layers.MaxPooling2D(\n",
    "    pool_size=(2, 2),   # Size of the pooling window (2x2)\n",
    "    strides=(2, 2)      # Strides for the pooling operation (2 pixels at a time)\n",
    "))\n",
    "\n",
    "model.add(tf.keras.layers.Convolution2D(\n",
    "    kernel_size=5,\n",
    "    filters=16,\n",
    "    strides=1,\n",
    "    activation=tf.keras.activations.relu,\n",
    "    kernel_initializer=tf.keras.initializers.VarianceScaling()\n",
    "))\n",
    "\n",
    "model.add(tf.keras.layers.MaxPooling2D(\n",
    "    pool_size=(2, 2),\n",
    "    strides=(2, 2)\n",
    "))\n",
    "\n",
    "# Add a Flatten layer to the neural network model\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "# Add a Dense (fully connected) layer to the neural network model\n",
    "model.add(tf.keras.layers.Dense(\n",
    "    units=128,                      # Number of neurons (units) in the layer\n",
    "    activation=tf.keras.activations.relu  # Activation function (Rectified Linear Unit - ReLU)\n",
    "))\n",
    "\n",
    "# Add a Dropout layer to the neural network model\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "\n",
    "# Add a Dense (fully connected) output layer to the neural network model\n",
    "model.add(tf.keras.layers.Dense(\n",
    "    units=10,                             # Number of output units (e.g., for classification)\n",
    "    activation=tf.keras.activations.softmax,  # Activation function (softmax for multi-class classification)\n",
    "    kernel_initializer=tf.keras.initializers.VarianceScaling()  # Weight initialization\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "78af5f92a7b37438",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-24T19:07:04.228172700Z",
     "start_time": "2023-09-24T19:06:44.211387700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_4 (Conv2D)           (None, 24, 24, 8)         208       \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPoolin  (None, 12, 12, 8)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 8, 8, 16)          3216      \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPoolin  (None, 4, 4, 16)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 37610 (146.91 KB)\n",
      "Trainable params: 37610 (146.91 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fefacd7f6d866c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
